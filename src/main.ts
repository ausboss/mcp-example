// main.ts

import { convertToOpenaiTools } from "./utils/toolHelpers.js";
import { createMcpClient } from "./utils/mcpClient.js";
import { fetchTools } from "./utils/toolHelpers.js";
import { processOllamaToolCalls } from "./utils/ollamaHelpers.js";

async function runOllamaWithMcpTools(model: string, initialPrompt: string) {
  let client, transport;

  try {
    const mcpClientResult = await createMcpClient();
    client = mcpClientResult.client;
    transport = mcpClientResult.transport;

    const mcpTools = await fetchTools(client);
    console.log("\nğŸ“š Available MCP Tools:", JSON.stringify(mcpTools, null, 2));

    if (!mcpTools) {
      console.log("âŒ No tools fetched from MCP.");
      return;
    }

    const ollamaTools = convertToOpenaiTools(mcpTools);
    // console.log(
    //   "\nğŸ› ï¸ Converted Ollama Tools:",
    //   JSON.stringify(ollamaTools, null, 2)
    // );

    console.log("\nğŸš€ Starting task with prompt:", initialPrompt);

    const result = await processOllamaToolCalls(
      model,
      initialPrompt,
      ollamaTools,
      client
    );

    if (result.endsWith("<END>")) {
      console.log("\nâœ… Task completed successfully!");
      console.log("ğŸ“„ Final result:", result.replace("<END>", ""));
    } else {
      console.log("\nâš ï¸ Task ended without proper completion marker");
    }
  } catch (error: any) {
    console.error("\nâŒ An error occurred:", error);
  } finally {
    if (client) await client.close();
    if (transport) await transport.close();
    process.exit(0);
  }
}

// Open-ended prompt that lets the model decide how to solve the task
const initialPrompt = `Tell me what the file stored in 'test-files' says.`;

runOllamaWithMcpTools("qwen2.5:latest", initialPrompt).catch((error) =>
  console.error("An error occurred:", error)
);
